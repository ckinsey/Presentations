<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Building Data Pipelines in Python</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/lofty.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
				    <h1>Building Data Pipelines with Python</h1>
				</section>
				
				<section>
				    <section>
                        <h2>Casey Kinsey</h2>
                        <p><a href="https://twitter.com/cordiskinsey/" target=_blank>@quesokinsey</a></p>
                        <p><a href="https://github.com/ckinsey" target=_blank>github.com/ckinsey</a></p>
                        <p><a href="http://linkedin.com/in/caseykinsey" target=_blank>linkedin.com/in/caseykinsey</a></p>
                    </section>
                    <section data-background="#34b6e6">
                        <div style="background: url(assets/lofty_logo_dark.png) center no-repeat; height: 300px;"></div>
                        <p><a href="http://hirelofty.com" target=_blank style="color: #FFFFFF;">hirelofty.com</a></p>
                        <p><a href="http://github.com/loftylabs" target=_blank style="color: #FFFFFF;">http://github.com/loftylabs</p>
                    </section>
                    <section>
                        <h2>Get These Slides</h2>
                        <p><a href="http://hirelofty.com/djangocon14" target=_blank>hirelofty.com/djangocon14</a></p>
                    </section>
				    
				</section>
				<section>
				    <section>
                        <h2>What is a Data Pipeline</h2>
                        <h3>And When Do I Need It?</h3>
                        <p class="fragment">A data pipeline is a software system consistently and programmatically moving data out of a datastore, manipulating it, and moving it back to a datastore.<p>
                    </section>

                    <section>
                    	<h3>Use Cases for Data Pipelines</h3>
                    	<ul>
                    		<li>Data Ingestion</li>
                    		<li>Data Consolidation</li>
                    		<li>Data Cleansing</li>
                    		<li>Data Analysis</li>
                    	</ul>
                    	<br><br><br>
                        <p><strong>Particularly</strong> when:<br>
                        	<ul>
	                         	<li>Timing is important (real time)</li>
		                        <li>Your data is too large for hands-on analysis.</li>
		                    </ul>
		                </p>
                    </section>
                </section>

				<section>
				    <section>
        				<h2>Why Python?</h2>
        		    </section>
        		    <section>
        		        <h3>Python's Thriving Ecosystem</h3>
        		        <p>Python's ecosystem includes many world-class open source modules that are well geared-towards data manipulation</p>
        		        <ul class="fragment">
            		        <li>Pandas:  The defacto standard data analysis library for Python</li>
            		        <li>Numpy:  Scientific computing fundamentals</li>
            		        <li>Numba:  JIT compiling to LLVM code</li>
            		        <li>Pint, requests, Scrapy, SQLAlchemy, etc. </li>
            		    </ul>

                    </section>
        		    <section>
        		        <h3>Distributability / Portability</h3>
        		        <ul>
        		        	<li>High end web frameworks:  Django, Flask, Pyramid</li>
        		        	<li>A strong learning language (very human readable)</li>
        		        	<li>Highly maintainable</li>
        		        </ul>
        		        	
                    </section>
                </section>
                <section>
                    <section>
                    	<h2>The ETL Pattern</h2>
                    	<p>ETL (Extract/Expand, Transform, Load) is an architecture pattern for moving data from one or more systems into another.</p>
               			<ul>
							<li class="fragment"><strong>Extraction</strong> is the process by which data is retrieved from source datastores.</li>
							<li class="fragment"><strong>Transformation</strong> is where all calculations, type conversions, etc are performed.</li>
							<li class="fragment"><strong>Loading</strong> is the process by which transformed data is sent to output, or target datastores.</li>
						</ul>
               		</section>
               		<section>
               			<h3>ETL Ground Rules</h3>
               			<ul>
							<li>Extraction, Transformation, and Loading should be isolated, independently runnable processes.</li>
							<li>Extract is only for copying data.</li>
							<li>Transform is the only place where data is <em>changed</em>.</li>
							<li>Loading is a push-only mechanism for transformed data.</li>
						</ul>
               		</section>
               		<section>
               			<h3>Intermediate Data Storage</h3>
               			<p>To separate extract, transform, and load, the data has to go somewhere.<p>
               			<ul>
							<li>Memory can work for lightweight / simple pipelines.</li>
							<li>Persistent/non-memory storage is critical for big pipelines<br>
							    (throughput constraints, clustering, and general flexibility).</li>
						</ul>
               		</section>
                </section>
                <section>
                    <section>
                        <h2>Extract</h2>
                        <p>Key Focus:  Simplicity</p>
                        <ul>
                            <li>Just read the data</li>
                            <li>Don't make changes or format</li>
                            <li>Let extraction be flexible</li>
                        </ul>
                    </section>
                    <section>
                        <h3>Extract:  Intermediate Storage Formats</h3>
                        <p>Many options to choose from, and none are always the right answer.  This is esoteric to your dataset.</p>
                        <ul>
                            <li>JSON<ul><li>
                                Does not store much information about datatypes,<br> 
                                which is good for extracted data.<br>
                                Easy to parse with Python.
                                <pre><code>import json
                                
python_dict = json.loads(json_data)
                                </code></pre>
                                </li></ul>
                                
                            </li>
                            <li>CSV<ul><li>
                                Easily parsed with Pandas.<br>
                                Can be very useful for aggregations in the upcoming<br> 
                                Transform step</li></ul>
                            </li>
                            <li>Non-relational Datastore<ul><li>
                                Fast, makes a great storage system for the above formats.
                                </li></ul>
                            </li>
                            <li>Relational Database<ul><li>
                                Highly persistent and easy to select one entry<br> 
                                at a time, but slow to write.<br>
                                Let all columns be nullable and use varchars / text.</li></ul>
                            </li>
                            
                        </ul>
                    </section>
                    
                </section>
                <section>
                    <section>
                        <h2>Transform</h2>
                        <p>Key Focus:  Atomicity</p>
                        
                        <p>atomicity:  the state or fact of being composed of indivisible units.</p>
                        <ul>
                            <li>Transforming data should work on the smallest chunks possible.</li>
                            <li>Complete or fail, no half-transformed state.</li>
                        </ul>

                    </section>
                    <section>
                        <h3>Transform: Intermediate Storage Formats</h3>
                        <p>Be more picky about storage formats for transformed data.</p>
                        <ul>
                            <li>JSON / CSV<ul><li>
                                Still easily parsed by Python.<br>
                                No built-in way of keeping your data typed.</li></ul>
                            </li>
                            <li>Python Pickles<ul><li>
                                Easy to store and un-pickle.<br>
                                Retains a perfect Pythonic representation of transformed data.
                                </li></ul>
                            </li>
                            <li>
                                Non-relational Datastore<ul><li>
                                    Fast, but likely requires heavy effort to maintain types.
                                </li></ul>
                            </li>
                            <li>Relational Database<ul><li>
                                Particularly useful if your target datastore is also a relational database.<br>
                                Let database schema reflect the same types as your target.  Speed is a downside.</li></ul>
                            </li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Load</h2>
                        <p>Key Focus: Idempotency</p>
                        <p>Idempotent functions/algorithms can run more than once and produce the same output.</p>
                        
                        <ul>
                            <li>If data is uniquely identifiable, it should be loaded uniquely.</li>
                            <li>Allows updates / deltas to be ETL'd with same pipeline.</li>
                            <li>Allows data to be extracted, transformed multiple times without duplication.</li>
                        </ul>

                    </section> 
                    <section>
                        <h2>Load</h2>
                        <p>Building Relationships</p>
                        <p>Loading relational data presents the extra challenge of rebuilding relationships</p>
                        <ul><li>Functional approach<ul>
                            <li>Identify if an atomic unit has un-resolved relationships.</li>
                            <li>Resolve relationships.</li>
                            <li>Bi-directionality is most efficient.</li>
                            </ul>
                            </li>
                        </ul>
                    </section> 
                </section>
                <section>
                    <section>
                        <h2>Clustering with Celery</h2>
                        <h3>Asynchronous Processing for Python</h3>
                    </section>
                    <section>
                        <h3>Celery</h3>
                        <p>"Celery is an asynchronous task queue/job queue based on distributed message passing."</p>
                        <p>Easily designate Python functions as asynchronous tasks.</p>
                        <p>Tasks are picked up by "worker" processes.  You can run many workers on many machines.</p>
                        <img src="https://wiki.openstack.org/w/images/b/b8/CeleryArch.jpg">
                    </section>
                    <section>
                        <h3>Converting to Celery Tasks</h3>
                        <p>Once Celery is configured, converting code to run on workers is easy.</p>
                        <pre><code>
from .celery import etl_app

@etl_app.task()
def extract():
    # ...
    
@etl_app.task()
def transform():
    # ...
    
@etl_app.task()
def load():
    # ...

extract.apply_async()
transform.apply_async()
load.apply_async()

                        </code></pre>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Tuning Your Cluster</h2>
                        <p>There are many ways to increase throughput in your distributed pipeline.</p>
                        <ul>
                            <li>Add workers
                                <ul>
                                    <li>Adding workers to the general pool will increase<br>
                                     the number of available threads for Extract/Transform/Load<br>
                                     uniformly</li>
                                </ul>
                            </li>
                            <li>Dedicate workers to tasks
                                <ul>
                                    <li>Celery allows you to setup multiple task queues in an<br>
                                    application.  Specific tasks can then be assigned to a<br>
                                    specific queue, and they will ignore tasks in other queues.
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Tuning Your Cluster</h3>
                        <p>Define which tasks are assigned to which queues in your Celery config<p>
                        <pre><code>
# celery.py

CELERY_ROUTES = {
    # Dotted path to your task function
    'etl.tasks.extract': {'queue': 'extract'},
    'etl.tasks.transform': {'queue': 'transform'},
    'etl.tasks.load': {'queue': 'load'},
}   

                        </code></pre>
                    </section>
                    <section>
                        <h3>Tuning Your Cluster</h3>
                        <p>When you deploy celery workers, specify the queue for each set of workers or each machine.<p>
                        <pre><code>
# start_celery.sh

# Start 4 threads on the extract queue
celery -A my_etl_pipeline worker --concurrency=4 -Q extract

# Start 8 threads on the transform queue
celery -A my_etl_pipeline worker --concurrency=8 -Q transform

# Start 4 threads on the load queue
celery -A my_etl_pipeline worker --concurrency=4 -Q load

                        </code></pre>
                    </section>
                </section>
                <section>
                    <h2>Where To Go From Here</h2>
                    <p>Some concepts to look into that weren't covered today</p>
                    <ul>
                        <li class="fragment">Read more about <a href="http://docs.celeryproject.org/en/latest/index.html">Celery</a>, namely Beat</li>
                        <li class="fragment">Add abstraction (with care) to Transformer classes.</li>
                        <li class="fragment">Explore relationship discovery in target datastores.</li>
                        <li class="fragment">Layer in analytics/reporting tools (ELK stack, Flower).</li>
                    </ul>
                </section>
                <section>
                    <h1>Thank You!</h1>
                    <p><a href="http://hirelofty.com/pipelines-with-python/" target=_blank>http://hirelofty.com/pipelines-with-python/</a></p>
                    <br><br>
                    <h2>Casey Kinsey</h2>
                    <p><a href="mailto:casey@hirelofty.com">casey@hirelofty.com</a></p>
                    <p><a href="http://twitter.com/cordiskinsey">@quesokinsey</a></p>
                    <p><a href="http://github.com/ckinsey">ckinsey on GitHub</a></p>
                </section>
    				
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

        <script>
        // Hold some markup that we might want to add back later
        /*<ul>
                            <li class="fragment"><em>Copy</em> the model the new app, create a schema migration.</li>
                            <li class="fragment"><em>Delete</em> the old model, create a schema migration.</li>
                            <li class="fragment"><em>Modify</em> your migration for creating the table to perform a <em>rename</em> operation.<ul>
                                <li><pre><code>
def forwards(self, orm):
    # db.create_table(u'newapp_mymodel', (
    #     ('description', self.gf('django.db.models.fields.TextField')()),
    #     (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
    # ))
    db.rename_table('oldapp_mymodel', 'newapp_mymodel') 

def backwards(self, orm):
    #db.delete_table('newapp_mymodel')     
    db.rename_table('newapp_mymodel', 'oldapp_mymodel')
    
                                </code></pre></li>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h2>Migrating Models Across Apps with South (continued)</h2>
                        <ul>
                            <li><em>Modify</em> the migration for deleting the old model to <em>depend on</em> your rename migration.
                                <pre><code>
class Migration(SchemaMigration):
    depends_on = (('newapp', '0001_initial'),)
    
    def forward(self):
        pass
        
    def backward(self):
        pass
                                </code></pre>
                            </li>
                            <li>Take extra care with ContentTypes and relationships.</li>
                        </ul>
                        <br><br>*/
        </script>

	</body>
</html>
