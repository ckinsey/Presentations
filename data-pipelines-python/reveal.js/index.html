<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>Building Data Pipelines in Python</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/lofty.css" id="theme">

		<!-- For syntax highlighting -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
				    <h1>Building Data Pipelines with Python</h1>
				</section>
				
				<section>
				    <section>
                        <h2>Casey Kinsey</h2>
                        <p><a href="https://twitter.com/cordiskinsey/" target=_blank>@quesokinsey</a></p>
                        <p><a href="https://github.com/ckinsey" target=_blank>github.com/ckinsey</a></p>
                        <p><a href="http://linkedin.com/in/caseykinsey" target=_blank>linkedin.com/in/caseykinsey</a></p>
                    </section>
                    <section data-background="#34b6e6">
                        <div style="background: url(assets/lofty_logo_dark.png) center no-repeat; height: 300px;"></div>
                        <p><a href="http://hirelofty.com" target=_blank style="color: #FFFFFF;">hirelofty.com</a></p>
                        <p><a href="http://github.com/loftylabs" target=_blank style="color: #FFFFFF;">http://github.com/loftylabs</p>
                    </section>
                    <section>
                        <h2>Get These Slides</h2>
                        <p><a href="http://hirelofty.com/djangocon14" target=_blank>hirelofty.com/djangocon14</a></p>
                    </section>
				    
				</section>
				<section>
				    <section>
                        <h2>What is a Data Pipeline</h2>
                        <h3>And When Do I Need It?</h3>
                        <p class="fragment">A data pipeline is a software system consistently and programmatically moving data from one datastore to another.<p>
                    </section>

                    <section>
                    	<h3>Use Cases for Data Pipelines</h3>
                    	<ul>
                    		<li class="fragment">External data ingestion</li>
                    		<li class="fragment">Internal data consolidation</li>
                    		<li class="fragment">Data cleansing</li>
                    	</ul>
                    	<br><br><br>
                        <p class="fragment"><strong>Particularly</strong> when:<br>
                        	<ul class="fragment">
	                         	<li>timing is important</li>
		                        <li>your data is too large for hands-on analysis.</li>
		                    </ul>
		                </p>
                    </section>
                </section>

				<section>
				    <section>
        				<h2>Why Python?</h2>
        		    </section>
        		    <section>
        		        <h3>Python's Thriving Ecosystem</h3>
        		        <p>Python's ecosystem includes many world-class open source modules that are well geared-towards data manipulation</p>
        		        <ul class="fragment">
            		        <li>Pandas:  The defacto standard data analysis library for Python</li>
            		        <li>Numpy:  Scientific computing fundamentals</li>
            		        <li>Numba:  JIT compiling to LLVM code</li>
            		        <li>Pint, requests, Scrapy, SQLAlchemy, etc. </li>
            		    </ul>

                    </section>
        		    <section>
        		        <h3>Distributability / Portability</h3>
        		        <ul>
        		        	<li>High end web frameworks:  Django, Flask, Pyramid</li>
        		        	<li>A strong learning language (very human readable)</li>
        		        	<li>Highly maintainable</li>
        		        </ul>
        		        	
                    </section>
                </section>
                <section>
                    <section>
                    	<h2>The ETL Pattern</h2>
                    	<p>ETL (Extract, Transform, Load) is an architecture pattern for moving data from one or more systems into another.</p>
               			<ul>
							<li><strong>Extraction</strong> is the process by which data is retrieved from source datastores.</li>
							<li><strong>Transformation</strong> is where all calculations, type conversions, etc are performed.</li>
							<li><strong>Loading</strong> is the process by which transformed data is sent to output, or target datastores.</li>
						</ul>
               		</section>
               		<section>
               			<h3>ETL Fundamentals</h3>
               			<ul>
							<li>Extraction, Transformation, and Loading should be isolated, independent processes.  As such, after it process completes data needs to be stored in an intermediate storage format.</li>
							<li>Extract is only for copying, not coercing!</li>
							<li>Transform is where all calculations and manipulation of the data happens.</li>
							<li>Loading is a publishing mechanism for transformed data.</li>
						</ul>
               		</section>
                </section>
                <section>
                    <section>
                        <h2>Extract</h2>
                        <p>A base class lets you enforce behavior across all ETL Extract modules.</p>
                        <pre><code>
from logging import getLogger

logger = getLogger('etl-extract')

class BaseExtractor(object):
    """ Defines base behavior for all extractors """
    
    def store(self, extracted_data):
        """ Stores extracted data (a dictionary) in intermediate storage """
        my_storage.write(extracted_data)
        
    def execute_extractions(self):
        raise NotImplementedError()
        
    def extract(self):
        logger.info("Extraction starting")
        self.execute_extractions()
        logger.info("Extraction completed")
		
		</code></pre>
                    </section>
                    <section>
                        <h2>Extract: Extract Classes</h2>
                        <p>Build an Extractor class per-datatype<p>
						<pre><code>


class EmployeeExtractor(BaseExtractor):
    """ Extracts employee records from a CSV """

    def __init__(self, *args, **kwargs):
        self.extract_path = args[0]
        super().__init__()
    
    def execute_extractions(self):    
        with open(self.extract_path, 'r') as csv_file:
            dataframe = read_csv(csv_file)
            for idx, entry in dataframe.iterrows():
                self.store(entry.to_dict())
        

                </code></pre>
                    </section>
                    <section>
                        <h3>Extract:  Data Source Formats</h3>
                        <p>If you have many sources, it may make sense to build base classes for each type of source datastore.<p>
                        <ul>
                            <li>
                            	File-based:
                            	<ul>
                            		<li>CSVExtractor</li>
                            		<li>ExcelExtractor</li>
                            		<li>ShapefileExtractor</li>
                            	</ul>
                            </li>
                            <li>
                            	Databases: 
                            	<ul>
                            		<li>SQLExtractor</li>
                            		<li>MongoDBExtractor</li>
                            	</ul>
                            </li>
                            <li>
                            	HTTP APIs:
                            	<ul>
                            		<li>RESTExtractor</li>
                            		<li>SOAPExtractor</li>
                            	</ul>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Extract:  Intermediate Storage Formats</h3>
                        <p>Many options to choose from, and none are always the right answer.  This is esoteric to your dataset.</p>
                        <ul>
                            <li>JSON<ul><li>
                                Does not store much information about datatypes,<br> 
                                which is good for extracted data.<br>
                                Easy to parse with Python.</li></ul>
                            </li>
                            <li>CSV<ul><li>
                                Easily parsed with Pandas.<br>
                                Can be very useful for aggregations in the upcoming<br> 
                                Transform step</li></ul>
                            </li>
                            <li>Structured Database<ul><li>
                                Highly persistent and easy to select one entry<br> 
                                at a time, but slow to write.<br>
                                Let all columns be nullable and use varchars / text.</li></ul>
                            </li>
                            
                        </ul>
                    </section>
                    
                </section>
                <section>
                    <section>
                        <h2>Transform</h2>
                        <p>A base class lets you enforce behavior across all ETL Transform modules.</p>
                        <pre><code>
class BaseTransformer:
    """The BaseTransformer enforces basic behavior of transformations on extracted data."""

    def __init__(self, *args, **kwargs):
        """ Retrieve the object which we intend to transform. """
        self.extracted_data = args[0]
        super().__init__()
        
    def store(self, transformed_data):
        """ Writes transformed_data to intermediate storage """
        storage_instance.write(transformed_data)
        
    def execute_transformations(self):
        raise NotImplementedError()
        
    def transform(self):
        logger.info("Transformation starting")
        self.execute_transformations()
        logger.info("Transformation completed")
                            </code></pre>
                    </section>
                    <section>
                        <h3>Transform: Transformer Classes</h3>
                        <p>Create a transformer per-datatype</p>
                        <pre><code>
class EmployeeTransformer(BaseTransformer):

    def execute_transformations(self):
        ex_data = self.extracted_data
        
        transformed_data = {
            'employee_id': int(ex_data['employee_id'],
            'name': str(ex_data['name'],
        }
        
        lat, long = geocode_address(ex_data['address'])
        transformed_data.update({
            'lat': lat,
            'long': long,
        })
        
        return transformed_data
                            </code></pre>
                    </section>
                    <section>
                        <h3>Transform: Intermediate Storage Formats</h3>
                        <p>Be more picky about storage formats for transformed data.</p>
                        <ul>
                            <li>JSON / CSV<ul><li>
                                Still easily parsed by Python.<br>
                                No built-in way of keeping your data typed.</li></ul>
                            </li>
                            <li>Python Pickles<ul><li>
                                Easy to store and un-pickle.<br>
                                Retains a perfect Pythonic representation of transformed data.
                                </li></ul>
                            </li>
                            <li>Structured Database<ul><li>
                                Particularly useful if your target datastore is a DB.<br>
                                Let database schema reflect the same types as your target.</li></ul>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Transform: Intermediate Storage Formats</h3>
                        <p>Be more picky about storage formats for transformed data.</p>
                        <ul>
                            <li>JSON / CSV<ul><li>
                                Still easily parsed by Python.<br>
                                No built-in way of keeping your data typed.</li></ul>
                            </li>
                            <li>Python Pickles<ul><li>
                                Easy to store and un-pickle.<br>
                                Retains a perfect Pythonic representation of transformed data.
                                </li></ul>
                            </li>
                            <li>Structured Database<ul><li>
                                Particularly useful if your target datastore is a DB.<br>
                                Let database schema reflect the same types as your target.</li></ul>
                            </li>
                        </ul>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Load</h2>
                        <p>A base class enforces behavior across all ETL Load modules</p>
                        <pre><code>
                            
class BaseLoader(object):
    """ The BaseLoader enforces basic behavior of loading with transformed data """
    
    target_model = None
    idempotent_on = []

    def get_model(self):
        """ Returns an ORM Model for the target storage. """
        return self.target_model
        
    def execute_loading(self, transformed_data):
        raise NotImplementedError() 

    def load(self, transformed_data):
        logger.info('Load starting')
        self.execute_loading(transformed_data)
        logger.info('Load completed')
        </code></pre>
                    </section>
                    <section>
                        <h3>Load: Loader Classes</h2>
                        <p>Build a Loader class per-datatype<p>
                        <pre><code>
                            
class EmployeeLoader(object):
    """ The EmployeeLoader loads transformed employee data into a Django model """
    
    target_model = EmployeeModel
        
    def execute_loading(self, transformed_data):
        """ A non-idempotent example of loading """
        
        model = self.get_model()
        instance = model(**transformed_data) 
        instance.save()   
        
        </code></pre>
                    </section>
                    <section>
                        <h3>Load: Idempotency</h2>
                        <p>Idempotent loading is a big advantage in an ETL pipeline<p>
                        <pre><code>
class EmployeeLoader(object):
    """ The EmployeeLoader loads transformed employee data into a Django model """
    
    target_model = EmployeeModel
    idempontent_on = ['employee_id']
        
    def execute_loading(self, transformed_data):
        """ A idempotent example of loading """
        model = self.get_model()

        lookup_kwargs = {k: transformed_data[k] for k in self.idempontent_on}
        instance, was_created = model.objects.get_or_create(**lookup_kwargs)
        # get_or_create(employee_id=transformed_data['employee_id'])

        for k, v in transformed_data.items():
            setattr(instance, k, v)        
        instance.save()
        
        </code></pre>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Tying It Together</h2>
                        <p>Let's look at how our ETL pipeline could be run.</p>
                        <pre><code>                        
def extracted_pop():
    # ...
    
def transformed_pop():
    # ...        
    
def run_etl():
    """ Executes and manages the ETL pipeline """
    
    extractor = EmployeeExtractor('employees.csv')
    extractor.extract()
    
    transformer = EmployeeTransformer(extracted_pop())
    transformer.transform()
        
    loader = EmployeeLoader(transformed_pop())
    loader.load()
    </code></pre>
                    </section>
                    <section>
                        <h3>Tying It Together</h3>
                        <p class="fragment">Our example demonstrates the flow of one item through ETL pipeline.</p>
                        <p class="fragment">Still synchronous--loading executes after transform.</p>
                        <p class="fragment">How can we make each process run asynchronously?</p>    
                    </section>
                </section>
                <section>
                    <section>
                        <h2>Celery</h2>
                        <h3>Asynchronous Processing for Python</h3>
                    </section>
                    <section>
                        <h3>Celery</h3>
                        <p>"Celery is an asynchronous task queue/job queue based on distributed message passing."</p>
                        <p>Easily designate Python functions as asynchronous tasks.</p>
                        <p>Tasks are picked up by "worker" processes.  You can run many workers on many machines.</p>
                        <img src="https://wiki.openstack.org/w/images/b/b8/CeleryArch.jpg">
                    </section>
                    <section>
                        <h3>Celery: Defining Tasks</h3>
                        <p>Let's imagine how we'd divide our example ETL pipeline into tasks<p>
                        <pre><code>
# tasks.py
from .celery import etl_app

@etl_app.task
def extract(filename):
    e = EmployeeExtractor(filename)
    e.extract()

@etl_app.task
def transform(ed_key):
    t = EmployeeTransformer(ed_key)
    t.transform()

@etl_app.task
def load(td_key):
    l = EmployeeLoader(td_key)
    l.load()
                        </code></pre>
                    </section>
                    <section>
                        <h3>Celery: Calling Tasks</h3>
                        <p>Now lets imagine how we'd schedule tasks from our pipeline.</p>
                        <pre><code>
# extract.py
from .tasks import transform
                        
class BaseExtractor(object):
    """ Defines base behavior for all extractors """
    
    def store(self, extracted_data):
        """ Stores extracted data (a dictionary) in intermediate storage """
        ed_key = my_storage.write(extracted_data)
        
        # Schedule a task for the transformation of this data
        tansform.delay(ed_key)
        
                        </code></pre>
                    </section>
                    <section>
                        <h3>Celery: Calling Tasks</h3>
                        <p>On the transformer side of things...</p>
                        <pre><code>
# trasform.py
from .tasks import load
                        
class BaseTransformer(object):
    """ Defines base behavior for all extractors """
    
    def __init__(self, ed_key, *kwargs):
        """ Look up data in intermediate storage based on key in ed_key """
        self.extracted_data = extracted_lookup(ed_key)
        
    def store(self, transformed_data):
        td_key = storage_instance.write(transformed_data)
        load.delay(td_key)
                        </code></pre>
                        <p>And apply similar logic to the BaseLoader class</p>
                    </section>
                    <section>
                        <h3>Celery: Asynchronous ETL</h3>
                        <p>Now how is our process executed?</p>
                        <pre><code>
from .tasks import extract        
extract.delay('employees.csv')
                        </code></pre>
                        <p>The task queue will fill with transform/load tasks until they are exhausted by the worker pool.</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h2>ETL Registry</h2>
                        <h3>Managing Many datatypes</h3>
                    </section>
                    <section>
                        <h3>ETL Registry: Architecture</h3>
                        <p>Implementing a registry pattern allows us to broaden the pipeline with many ETL sources.</p>
                        <p>The relationships between Extract, Transform, and Load classes are stored at runtime.</p>
                        <p>A reference to the appropriate class can be linked to records in intermediate storage.</p>
                        
                        <p>https://gist.github.com/ckinsey/762394f280dec30f012b</p>
                    </section>
                    <section>
                        <h3>ETL Registry: Looking Up Classes</h3>
                        <p>Let's see how this can be applied to a Celery task</p>
                        
                        <pre><code>
# tasks.py
from .celery import etl_app
from .etl_registry import registry

@etl_app.task
def transform(ed_key, transformer_name):
    """ Look up the transformer referenced by name and instantiate / execute it """
    
    entry = registry.get_entry(transformer=transformer_name)
    transformer_class = entry['class']

    t = transformer_class(ed_key)
    t.transform()
    </code></pre>
                    </section>
                    <section>
                        <h3>ETL Registry: Looking Up Classes</h3>
                        <p>How did the task receive the transformer name?</p>
                        
                        <pre><code>
from .etl_registry import registry

class BaseExtractor(object):
    """ Defines base behavior for all extractors """
    
    def store(self, extracted_data):
        """ Stores extracted data (a dictionary) in intermediate storage """
        ed_key = my_storage.write(extracted_data)
        
        # Look up transformer name
        entry = registry.get_entry(extractor=self.__name__)
        
        # Schedule a task for the transformation of this data
        transform.delay(ed_key, entry['transformer']['name'])
        </code></pre>   
                    </section>
                    <section>
                        <h3>ETL Registry: Running ETL</h3>
                        <p>Kicking off our ETL pipeline is still simple.</p>
                        <pre><code>
from etl_registry import registry
from .tasks import extract        

def run_etl():
    for entry in registry._registry:
        e = entry['extractor']['class']()
        e.extract()
                        </code></pre>
                    </section>
                    
                </section>
                <section>
                    <section>
                        <h2>Tuning Your Cluster</h2>
                        <p>There are many ways to increase throughput in your distributed pipeline.</p>
                        <ul>
                            <li>Add workers
                                <ul>
                                    <li>Adding workers to the general pool will increase<br>
                                     the number of available threads for Extract/Transform/Load<br>
                                     uniformly</li>
                                </ul>
                            </li>
                            <li>Dedicate workers to tasks
                                <ul>
                                    <li>Celery allows you to setup multiple task queues in an<br>
                                    application.  Specific tasks can then be assigned to a<br>
                                    specific queue, and they will ignore tasks in other queues.
                                    </li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h3>Tuning Your Cluster</h3>
                        <p>Define which tasks are assigned to which queues in your Celery config<p>
                        <pre><code>
# celery.py

CELERY_ROUTES = {
    # Dotted path to your task function
    'etl.tasks.extract': {'queue': 'extract'},
    'etl.tasks.transform': {'queue': 'transform'},
    'etl.tasks.load': {'queue': 'load'},
}   

                        </code></pre>
                    </section>
                    <section>
                        <h3>Tuning Your Cluster</h3>
                        <p>When you deploy celery workers, specify the queue for each set of workers or each machine.<p>
                        <pre><code>
# start_celery.sh

# Start 4 threads on the extract queue
celery -A my_etl_pipeline worker --concurrency=4 -Q extract

# Start 8 threads on the transform queue
celery -A my_etl_pipeline worker --concurrency=8 -Q transform

# Start 4 threads on the load queue
celery -A my_etl_pipeline worker --concurrency=4 -Q load

                        </code></pre>
                    </section>
                </section>
                <section>
                    <h2>Where To Go From Here</h2>
                    <p>Some concepts to look into that weren't covered today</p>
                    <ul>
                        <li class="fragment">Read more about <a href="http://docs.celeryproject.org/en/latest/index.html">Celery</a>, namely Beat</li>
                        <li class="fragment">Add abstraction (with care) to Transformer classes.</li>
                        <li class="fragment">Explore relationship discovery in target datastores.</li>
                        <li class="fragment">Layer in analytics/reporting tools (ELK stack, Flower).</li>
                    </ul>
                </section>
                <section>
                    <h1>Thank You!</h1>
                    <p><a href="http://hirelofty.com/pipelines-with-python/" target=_blank>http://hirelofty.com/pipelines-with-python/</a></p>
                    <br><br>
                    <h2>Casey Kinsey</h2>
                    <p><a href="mailto:casey@hirelofty.com">casey@hirelofty.com</a></p>
                    <p><a href="http://twitter.com/cordiskinsey">@quesokinsey</a></p>
                    <p><a href="http://github.com/ckinsey">ckinsey on GitHub</a></p>
                </section>
    				
			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>

		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,
				slideNumber: true,

				theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'default', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
				// parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
				// parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

        <script>
        // Hold some markup that we might want to add back later
        /*<ul>
                            <li class="fragment"><em>Copy</em> the model the new app, create a schema migration.</li>
                            <li class="fragment"><em>Delete</em> the old model, create a schema migration.</li>
                            <li class="fragment"><em>Modify</em> your migration for creating the table to perform a <em>rename</em> operation.<ul>
                                <li><pre><code>
def forwards(self, orm):
    # db.create_table(u'newapp_mymodel', (
    #     ('description', self.gf('django.db.models.fields.TextField')()),
    #     (u'id', self.gf('django.db.models.fields.AutoField')(primary_key=True)),
    # ))
    db.rename_table('oldapp_mymodel', 'newapp_mymodel') 

def backwards(self, orm):
    #db.delete_table('newapp_mymodel')     
    db.rename_table('newapp_mymodel', 'oldapp_mymodel')
    
                                </code></pre></li>
                            </li>
                        </ul>
                    </section>
                    <section>
                        <h2>Migrating Models Across Apps with South (continued)</h2>
                        <ul>
                            <li><em>Modify</em> the migration for deleting the old model to <em>depend on</em> your rename migration.
                                <pre><code>
class Migration(SchemaMigration):
    depends_on = (('newapp', '0001_initial'),)
    
    def forward(self):
        pass
        
    def backward(self):
        pass
                                </code></pre>
                            </li>
                            <li>Take extra care with ContentTypes and relationships.</li>
                        </ul>
                        <br><br>*/
        </script>

	</body>
</html>
